{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "\n",
    "data = pd.read_csv(\"../data/kg_train.csv\")  # using the default encoding (utf-8,), works for ascii files\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sure -- bottom line - you need a special secur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dear Sir,I am Engr. Ugo Nzego with the Enginee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abedin Huma &lt;AbedinH@state.gov&gt;Saturday Novemb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There is an Oct 16th George Marshall event at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;P&gt;1 25% for you as the account owner &lt;BR&gt;2 65...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>STRONG&gt;&lt;A href=3D\"http://www.cnn.com/2003/WORL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dear Friend,My name is Edward Moore QC.Princip...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                            Will do.      0\n",
       "2   Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3   Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                 fyi      0\n",
       "5   sure -- bottom line - you need a special secur...      0\n",
       "6   Dear Sir,I am Engr. Ugo Nzego with the Enginee...      1\n",
       "7   Abedin Huma <AbedinH@state.gov>Saturday Novemb...      0\n",
       "8   There is an Oct 16th George Marshall event at ...      0\n",
       "9   <P>1 25% for you as the account owner <BR>2 65...      1\n",
       "10  STRONG><A href=3D\"http://www.cnn.com/2003/WORL...      1\n",
       "11  Dear Friend,My name is Edward Moore QC.Princip...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(\"label\", axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/00w2xr_j197gk698c1ljm3300000gn/T/ipykernel_10709/3287802385.py:4: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "/var/folders/x1/00w2xr_j197gk698c1ljm3300000gn/T/ipykernel_10709/3287802385.py:18: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  clean_text = BeautifulSoup(clean_text, \"html.parser\").text\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def clean_html_full(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Remove script and style tags\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    # Remove comments\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "\n",
    "    # Get text with spaces and decode HTML entities\n",
    "    clean_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    # Also, decode HTML entities\n",
    "    clean_text = BeautifulSoup(clean_text, \"html.parser\").text\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "X_train[\"clean_text\"] = X_train[\"text\"].apply(clean_html_full)\n",
    "X_test[\"clean_text\"] = X_test[\"text\"].apply(clean_html_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "\n",
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_preprocess(text):\n",
    "    # Remove special characters and numbers\n",
    "    # note: we'll keep also the characters € and $, as they can be relevant for the analysis\n",
    "    text = re.sub(r\"[^a-zA-Z\\s€$]\", \" \", text)\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = re.sub(r\"\\s\\w\\s\", \" \", text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r\"^\\w\\s\", \"\", text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r\"^b\\s*\", \"\", text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "X_train[\"clean_text\"] = X_train[\"clean_text\"].apply(text_preprocess)\n",
    "X_test[\"clean_text\"] = X_test[\"clean_text\"].apply(text_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>regards mr nelson smith kindly reply me on my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>have not been able to reach oscar this am we a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>huma abedin i checking with pat on the will w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>can have it announced here on monday can today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>bank of africaagence san pedro bp san pedro co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>7653 2612ADAMA IBRAHIM________________________...</td>\n",
       "      <td>adama ibrahim tout savoir sur la curit de vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>What does that mean for our schedules?</td>\n",
       "      <td>what does that mean for our schedules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Dear Friend,My Compliment to you,I guess this ...</td>\n",
       "      <td>dear friend my compliment to you guess this le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...</td>\n",
       "      <td>dear president fdirector my name is mr micheal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Let me know if today or tomorrow works for you...</td>\n",
       "      <td>let me know if today or tomorrow works for you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...   \n",
       "535  I have not been able to reach oscar this am. W...   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...   \n",
       "557  I can have it announced here on Monday - can't...   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...   \n",
       "..                                                 ...   \n",
       "106  7653 2612ADAMA IBRAHIM________________________...   \n",
       "270             What does that mean for our schedules?   \n",
       "860  Dear Friend,My Compliment to you,I guess this ...   \n",
       "435  Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...   \n",
       "102  Let me know if today or tomorrow works for you...   \n",
       "\n",
       "                                            clean_text  \n",
       "29    regards mr nelson smith kindly reply me on my...  \n",
       "535  have not been able to reach oscar this am we a...  \n",
       "695   huma abedin i checking with pat on the will w...  \n",
       "557     can have it announced here on monday can today  \n",
       "836  bank of africaagence san pedro bp san pedro co...  \n",
       "..                                                 ...  \n",
       "106   adama ibrahim tout savoir sur la curit de vot...  \n",
       "270             what does that mean for our schedules   \n",
       "860  dear friend my compliment to you guess this le...  \n",
       "435  dear president fdirector my name is mr micheal...  \n",
       "102  let me know if today or tomorrow works for you...  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X_train[\"clean_text\"] = X_train[\"clean_text\"].apply(remove_stopwords)\n",
    "X_test[\"clean_text\"] = X_test[\"clean_text\"].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>regards mr nelson smith kindly reply private e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>huma abedin checking pat work jack jake rest a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>announced monday today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>7653 2612ADAMA IBRAHIM________________________...</td>\n",
       "      <td>adama ibrahim tout savoir sur la curit de votr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>What does that mean for our schedules?</td>\n",
       "      <td>mean schedules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Dear Friend,My Compliment to you,I guess this ...</td>\n",
       "      <td>dear friend compliment guess letter may come s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...</td>\n",
       "      <td>dear president fdirector name mr micheal ipenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Let me know if today or tomorrow works for you...</td>\n",
       "      <td>let know today tomorrow works would rather fin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...   \n",
       "535  I have not been able to reach oscar this am. W...   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...   \n",
       "557  I can have it announced here on Monday - can't...   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...   \n",
       "..                                                 ...   \n",
       "106  7653 2612ADAMA IBRAHIM________________________...   \n",
       "270             What does that mean for our schedules?   \n",
       "860  Dear Friend,My Compliment to you,I guess this ...   \n",
       "435  Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...   \n",
       "102  Let me know if today or tomorrow works for you...   \n",
       "\n",
       "                                            clean_text  \n",
       "29   regards mr nelson smith kindly reply private e...  \n",
       "535         able reach oscar supposed send pdb receive  \n",
       "695  huma abedin checking pat work jack jake rest a...  \n",
       "557                             announced monday today  \n",
       "836  bank africaagence san pedro bp san pedro cote ...  \n",
       "..                                                 ...  \n",
       "106  adama ibrahim tout savoir sur la curit de votr...  \n",
       "270                                     mean schedules  \n",
       "860  dear friend compliment guess letter may come s...  \n",
       "435  dear president fdirector name mr micheal ipenz...  \n",
       "102  let know today tomorrow works would rather fin...  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "X_train[\"clean_text\"] = X_train[\"clean_text\"].apply(lemmatize_text)\n",
    "X_test[\"clean_text\"] = X_test[\"clean_text\"].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>regard mr nelson smith kindly reply private em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>huma abedin checking pat work jack jake rest a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>announced monday today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>7653 2612ADAMA IBRAHIM________________________...</td>\n",
       "      <td>adama ibrahim tout savoir sur la curit de votr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>What does that mean for our schedules?</td>\n",
       "      <td>mean schedule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Dear Friend,My Compliment to you,I guess this ...</td>\n",
       "      <td>dear friend compliment guess letter may come s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...</td>\n",
       "      <td>dear president fdirector name mr micheal ipenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Let me know if today or tomorrow works for you...</td>\n",
       "      <td>let know today tomorrow work would rather find...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...   \n",
       "535  I have not been able to reach oscar this am. W...   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...   \n",
       "557  I can have it announced here on Monday - can't...   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...   \n",
       "..                                                 ...   \n",
       "106  7653 2612ADAMA IBRAHIM________________________...   \n",
       "270             What does that mean for our schedules?   \n",
       "860  Dear Friend,My Compliment to you,I guess this ...   \n",
       "435  Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...   \n",
       "102  Let me know if today or tomorrow works for you...   \n",
       "\n",
       "                                            clean_text  \n",
       "29   regard mr nelson smith kindly reply private em...  \n",
       "535         able reach oscar supposed send pdb receive  \n",
       "695  huma abedin checking pat work jack jake rest a...  \n",
       "557                             announced monday today  \n",
       "836  bank africaagence san pedro bp san pedro cote ...  \n",
       "..                                                 ...  \n",
       "106  adama ibrahim tout savoir sur la curit de votr...  \n",
       "270                                      mean schedule  \n",
       "860  dear friend compliment guess letter may come s...  \n",
       "435  dear president fdirector name mr micheal ipenz...  \n",
       "102  let know today tomorrow work would rather find...  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham:\n",
      "state        117\n",
      "pm            97\n",
      "would         94\n",
      "president     89\n",
      "mr            89\n",
      "time          81\n",
      "percent       80\n",
      "obama         77\n",
      "call          74\n",
      "secretary     74\n",
      "dtype: int64\n",
      "\n",
      "Top 10 words in spam:\n",
      "money          847\n",
      "account        743\n",
      "bank           646\n",
      "fund           626\n",
      "transaction    471\n",
      "business       424\n",
      "mr             423\n",
      "country        422\n",
      "million        370\n",
      "company        366\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Separate ham and spam\n",
    "ham_texts = X_train[y_train == 0][\"clean_text\"]\n",
    "spam_texts = X_train[y_train == 1][\"clean_text\"]\n",
    "\n",
    "# Fit on all training data\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train[\"clean_text\"])\n",
    "\n",
    "# Transform ham and spam\n",
    "ham_matrix = vectorizer.transform(ham_texts)\n",
    "spam_matrix = vectorizer.transform(spam_texts)\n",
    "\n",
    "# Get word counts\n",
    "ham_counts = pd.Series(ham_matrix.sum(axis=0).A1, index=vectorizer.get_feature_names_out())\n",
    "spam_counts = pd.Series(spam_matrix.sum(axis=0).A1, index=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Top 10 words in ham:\")\n",
    "print(ham_counts.sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nTop 10 words in spam:\")\n",
    "print(spam_counts.sort_values(ascending=False).head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "X_train['money_mark'] = X_train['clean_text'].str.contains(money_simbol_list, case=False)*1\n",
    "X_train['suspicious_words'] = X_train['clean_text'].str.contains(suspicious_words, case=False)*1\n",
    "X_train['text_len'] = X_train['clean_text'].apply(lambda x: len(x)) \n",
    "\n",
    "X_test['money_mark'] = X_test['clean_text'].str.contains(money_simbol_list, case=False)*1\n",
    "X_test['suspicious_words'] = X_test['clean_text'].str.contains(suspicious_words, case=False)*1\n",
    "X_test['text_len'] = X_test['clean_text'].apply(lambda x: len(x)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF shape: (800, 28041)\n",
      "Test TF-IDF shape: (200, 28041)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train[\"clean_text\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test[\"clean_text\"])\n",
    "\n",
    "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
    "print(\"Test TF-IDF shape:\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "['aa' 'aaa' 'aabeiawaeaambiqaceqedeqh' ... 'zzz' 'zzzahbxntxe' 'zzzj']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.toarray())\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "regard mr nelson smith kindly reply private email address nelsonsmith yahoo com\n",
      "\n",
      "\n",
      "Top words in first email:\n",
      "nelsonsmith: 0.4987\n",
      "nelson: 0.4492\n",
      "smith: 0.3709\n",
      "kindly: 0.2691\n",
      "yahoo: 0.2472\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Check TF-IDF on the first email\n",
    "#\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\")\n",
    "print(X_train.iloc[0][\"clean_text\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Top words first email\n",
    "row = X_train_tfidf[0]  # sparse vector for first email\n",
    "row_array = row.toarray().flatten()\n",
    "\n",
    "top_n = 5\n",
    "top_indices = row_array.argsort()[-top_n:][::-1]  # indices of top tf-idf scores\n",
    "top_words = [(feature_names[i], row_array[i]) for i in top_indices]\n",
    "\n",
    "print(\"Top words in first email:\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most important words overall:\n",
      "fyi: 34.4434\n",
      "money: 22.7986\n",
      "account: 21.0045\n",
      "bank: 20.3126\n",
      "fund: 18.1970\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Check most important words overall (sum tf-idf across all documents)\n",
    "# \n",
    "\n",
    "tfidf_sum = np.array(X_train_tfidf.sum(axis=0)).flatten()\n",
    "top_indices_overall = tfidf_sum.argsort()[-top_n:][::-1]\n",
    "top_words_overall = [(feature_names[i], tfidf_sum[i]) for i in top_indices_overall]\n",
    "\n",
    "print(\"\\nMost important words overall:\")\n",
    "for word, score in top_words_overall:\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Extra Task (optional) - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "Use a MultinimialNB with default parameters.\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results - Bag of Words only\n",
      "Accuracy:  0.9450\n",
      "Precision: 0.8721\n",
      "Recall:    1.0000\n",
      "F1-score:  0.9317\n",
      "\n",
      "Results - TF-IDF only\n",
      "Accuracy:  0.9350\n",
      "Precision: 0.8523\n",
      "Recall:    1.0000\n",
      "F1-score:  0.9202\n",
      "\n",
      "Results - Bag of Words + extra flags\n",
      "Accuracy:  0.8050\n",
      "Precision: 0.6607\n",
      "Recall:    0.9867\n",
      "F1-score:  0.7914\n",
      "\n",
      "Results - TF-IDF + extra flags\n",
      "Accuracy:  0.5550\n",
      "Precision: 0.4568\n",
      "Recall:    0.9867\n",
      "F1-score:  0.6245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Prepare CountVectorizer features\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(X_train[\"clean_text\"])\n",
    "\n",
    "X_train_bow = count_vectorizer.transform(X_train[\"clean_text\"])\n",
    "X_test_bow = count_vectorizer.transform(X_test[\"clean_text\"])\n",
    "\n",
    "# Prepare extra flags as dense arrays\n",
    "extra_train = X_train[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "extra_test = X_test[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "\n",
    "# Function to train and evaluate\n",
    "def train_eval(X_tr, X_te, y_tr, y_te, desc):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    prec = precision_score(y_te, y_pred)\n",
    "    rec = recall_score(y_te, y_pred)\n",
    "    f1 = f1_score(y_te, y_pred)\n",
    "    print(f\"Results - {desc}\")\n",
    "    print(f\"{'Accuracy:':<10} {acc:.4f}\")\n",
    "    print(f\"{'Precision:':<10} {prec:.4f}\")\n",
    "    print(f\"{'Recall:':<10} {rec:.4f}\")\n",
    "    print(f\"{'F1-score:':<10} {f1:.4f}\\n\")\n",
    "\n",
    "# 1) Bag of Words only\n",
    "train_eval(X_train_bow, X_test_bow, y_train, y_test, \"Bag of Words only\")\n",
    "\n",
    "# 2) TF-IDF only\n",
    "train_eval(X_train_tfidf, X_test_tfidf, y_train, y_test, \"TF-IDF only\")\n",
    "\n",
    "# 3) Bag of Words + extra flags\n",
    "X_train_bow_ext = hstack([X_train_bow, extra_train])\n",
    "X_test_bow_ext = hstack([X_test_bow, extra_test])\n",
    "train_eval(X_train_bow_ext, X_test_bow_ext, y_train, y_test, \"Bag of Words + extra flags\")\n",
    "\n",
    "# 4) TF-IDF + extra flags\n",
    "X_train_tfidf_ext = hstack([X_train_tfidf, extra_train])\n",
    "X_test_tfidf_ext = hstack([X_test_tfidf, extra_test])\n",
    "train_eval(X_train_tfidf_ext, X_test_tfidf_ext, y_train, y_test, \"TF-IDF + extra flags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis:\n",
    "#\n",
    "# - \"Bag of Words only\" performs best overall ⭐️ (highest precission, accuracy and F1 )\n",
    "# - \"TF-IDF only\" is close but slightly worse.\n",
    "# - Using the extra flags reduces performance significantly — might need feature scaling or different handling.\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "3.11.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
